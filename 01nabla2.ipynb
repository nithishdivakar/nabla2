{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nabla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pathlib import Path\n",
    "from typing import Any, Union, Tuple, Callable, TypeVar, Generic\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def force_list(itm):\n",
    "    if itm is None: \n",
    "        return None\n",
    "    if isinstance(itm,list):\n",
    "        itm_list = itm\n",
    "    elif isinstance(itm,tuple):\n",
    "        itm_list = [it for it in itm]\n",
    "    else: \n",
    "        itm_list = [itm,]\n",
    "    return itm_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def is_listy(x): \n",
    "    return isinstance(x, (list,tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def set_seed(seed_val=0):\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _progress(count, total, status='',time_info='',blank=False):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = \"{}% {}/{}\".format(round(100.0 * count / float(total), 1),count,total)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    \n",
    "    progress_str = '   {status} [{bar}] {percents}  {time_info}   \\r'.format(status=status,bar=bar,percents=percents,time_info=time_info)\n",
    "    if blank:\n",
    "        progress_str = \" \"*len(progress_str)+\"\\r\"\n",
    "    sys.stdout.write(progress_str)\n",
    "    sys.stdout.flush() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def format_time(t):\n",
    "    t = int(t)\n",
    "    h,m,s = t//3600, (t//60)%60, t%60\n",
    "    if h!= 0: return f'{h}:{m:02d}:{s:02d}'\n",
    "    else:     return f'{m:02d}:{s:02d}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ProgressBar():\n",
    "    def __init__(self, iterable, status=None):\n",
    "        self.iterable = iterable\n",
    "        self.len = len(iterable)\n",
    "        self.done = 0\n",
    "        self.status = status if status is not None else '>' \n",
    "\n",
    "    def __iter__(self):\n",
    "        start_t = time.time()\n",
    "        \n",
    "        for i, itm in enumerate(self.iterable):\n",
    "            yield itm\n",
    "            elapsed_t = time.time() - start_t\n",
    "            time_info = \"[{}/{}]\".format(format_time(elapsed_t),format_time(elapsed_t*self.len/(i+1)))\n",
    "            _progress(i, self.len, self.status, time_info)\n",
    "        _progress(i, self.len, self.status,time_info, blank=True)\n",
    "\n",
    "    def set_status(self, new_status):\n",
    "        self.status = new_status\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "for i in ProgressBar(range(15)):\n",
    "    time.sleep(0.5)\n",
    "    if i>5:time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# constants\n",
    "DISPLAY_DECIMALS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _set_device(device):\n",
    "    if device is None:\n",
    "        return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _prepare_batch(tb, device):\n",
    "    tb = force_list(tb)\n",
    "    tb = [t.to(device) if t is not None else None for t in tb]\n",
    "    return tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _batch_forward_pass(device, model, xb, yb,  compute_loss, loss_func=None):\n",
    "    # takes care of multiple input and or multiple output from model.\n",
    "    xb = _prepare_batch(xb, device)\n",
    "    pb = model(*xb)\n",
    "\n",
    "    if compute_loss  and (loss_func is not None):\n",
    "        yb = _prepare_batch(yb, device)\n",
    "        loss = loss_func(pb, *yb)\n",
    "    else:\n",
    "        loss=None\n",
    "\n",
    "    return loss, pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _show_loss_on_progress_bar(ds,loss)-> None:\n",
    "    if isinstance(ds,ProgressBar):\n",
    "        if isinstance(loss,(list,tuple)):\n",
    "            loss_val = loss[0].item()\n",
    "        else:\n",
    "            loss_val = loss.item()    \n",
    "        ds.set_status(\"{:4.2f}\".format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dataset_forward_pass(dataset, model, loss_func, is_train:bool, batch_preprocess_fn, prediction_recorder_fn, \n",
    "                          loss_pre_process_fn, gradient_post_proces_fn, optimizer, device )->None:\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    for data_batch in dataset:\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            phase = 'train' if is_train==True else 'eval'\n",
    "            xb, yb = batch_preprocess_fn(data_batch, phase=phase)\n",
    "            loss, pb = _batch_forward_pass(device, model, xb, yb, compute_loss=True, loss_func=loss_func)\n",
    "            prediction_recorder_fn(xb,yb,pb,loss,is_train=is_train)\n",
    "            _show_loss_on_progress_bar(dataset, loss)\n",
    "            if is_train:                \n",
    "                loss = loss_pre_process_fn(loss)\n",
    "                loss.backward()\n",
    "                gradient_post_proces_fn()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _recorder():\n",
    "    lut = {\n",
    "        'epoch':0,\n",
    "        'train_iter':0,\n",
    "        'valid_iter':0,\n",
    "        'train_losses':[],\n",
    "        'valid_losses':[],\n",
    "    }\n",
    "    \n",
    "    def get_loss_df(self):\n",
    "        train_loss_df = pd.DataFrame(self.lut['train_losses'],columns=['epoch_num','iter_num','loss','lr'])\n",
    "        valid_loss_df = pd.DataFrame(self.lut['valid_losses'],columns=['epoch_num','iter_num','loss','lr'])\n",
    "        \n",
    "        def _cleanup(df):        \n",
    "            f = pd.DataFrame(df.loss.to_list(),index= df.index)\n",
    "            f.columns = [f\"loss_{idx}\" for idx,col in enumerate(f.columns)]\n",
    "            x = pd.concat([df,f],axis=1)\n",
    "            x=x.drop(columns=['loss'])\n",
    "            return x\n",
    "        \n",
    "        return _cleanup(train_loss_df),_cleanup(valid_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _logger():\n",
    "    folder = None\n",
    "    log_file = None\n",
    "    \n",
    "    def __init__(self, exp_folder=None):\n",
    "        if exp_folder is not None:\n",
    "            self.folder = exp_folder\n",
    "        else:\n",
    "            self.folder = Path(\"debug\")\n",
    "            \n",
    "        self.log_file = self.folder/\"log.txt\"\n",
    "        \n",
    "        self.folder.mkdir(parents=True,exist_ok=True)\n",
    "        \n",
    "        self.__call__(\">\"*40)\n",
    "        self.__call__(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        \n",
    "    def __call__(self, msg):\n",
    "        if self.log_file:\n",
    "            with self.log_file.open(\"a\") as f:\n",
    "                f.write(msg)\n",
    "                f.write(\"\\n\")\n",
    "        sys.stdout.write(msg+\"\\n\")\n",
    "        sys.stdout.flush()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def _post_process_result(result):\n",
    "    \"\"\"\n",
    "    Floating point numbers with fixed precisions, zero padded on the right if required.\n",
    "    \"\"\"\n",
    "    f = lambda x: np.format_float_positional(x,precision=DISPLAY_DECIMALS)\n",
    "    vf = np.vectorize(f)\n",
    "    return vf(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export    \n",
    "def _eval_fn_on_predictions(eval_fn, predictions, ground_truth):\n",
    "    # get the functions's name\n",
    "    try:\n",
    "        name = eval_fn.__name__\n",
    "    except:\n",
    "        name = eval_fn.__class__.__name__\n",
    "\n",
    "    try:\n",
    "        result = eval_fn(predictions, ground_truth)\n",
    "        result = _post_process_result(result)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        \n",
    "    return name,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _eval_fns_on_ds(learner, ds, fns, info=''):\n",
    "    results = {}\n",
    "    predictions, ground_truth, loss_values = learner.predict(ds,info)\n",
    "    for fn in fns:\n",
    "        fn_name, result = _eval_fn_on_predictions(fn, predictions, ground_truth)\n",
    "        results[fn_name] = result\n",
    "        \n",
    "    loss_agg = np.asarray(loss_values).mean(axis=0)\n",
    "    results['loss'] = _post_process_result(loss_agg)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _base_learner():\n",
    "    eval_sets = defaultdict(list)\n",
    "    def __init__(self, device, exp_folder=None, eval_sets:dict=None):\n",
    "        \n",
    "        self.device = _set_device(device)\n",
    "        self.recorder = _recorder()\n",
    "        self.logger = _logger()\n",
    "        \n",
    "        self.eval_sets['valid'] = [] # do validation always. \n",
    "        \n",
    "        for set_name in eval_sets:\n",
    "            self.eval_sets[set_name].extend(force_list(eval_sets[set_name]))\n",
    "        \n",
    "    def on_backward_begin(self,loss):\n",
    "        if isinstance(loss,(list,tuple)): \n",
    "            return loss[0]\n",
    "        else:\n",
    "            return loss\n",
    "        \n",
    "    def on_backward_end(self):\n",
    "        return \n",
    "\n",
    "    def on_batch_begin(self, data_batch, phase='train'):\n",
    "        # do somthing . with a batch of data\n",
    "        # print(xb.mean(),xb.std(),xb.shape)\n",
    "        xb,yb = data_batch['x'] , data_batch['y'] \n",
    "        return xb,yb\n",
    "    \n",
    "    def on_batch_end(self,xb,yb,pb,loss,is_train=True):\n",
    "        # log losses    \n",
    "        if is_train:\n",
    "            lst = self.recorder.lut['train_losses']\n",
    "            epoch_num = self.recorder.lut['epoch']\n",
    "            iter_num  = epoch_num*len(self.data['train'])+self.recorder.lut['train_iter']\n",
    "        else:\n",
    "            lst = self.recorder.lut['valid_losses']\n",
    "            epoch_num = self.recorder.lut['epoch']\n",
    "            iter_num  = epoch_num*len(self.data['valid'])+self.recorder.lut['valid_iter']\n",
    "        if isinstance(loss,(list,tuple)):\n",
    "            loss = [l.tolist() for l in loss]\n",
    "        else: \n",
    "            loss = [loss.tolist(),]\n",
    "        \n",
    "        def get_lr():\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                return param_group['lr']\n",
    "        \n",
    "        lst.append({\n",
    "            'epoch_num':epoch_num,\n",
    "            'iter_num':iter_num,\n",
    "            'loss':loss,\n",
    "            'lr':get_lr(),\n",
    "        })\n",
    "        \n",
    "        # increment counters\n",
    "        if is_train==True: \n",
    "            self.recorder.lut['train_iter']+=1\n",
    "        else:\n",
    "            self.recorder.lut['valid_iter']+=1\n",
    "    \n",
    "    def on_epoch_end(self, ep_num):\n",
    "        epoch_num= self.recorder.lut['epoch']\n",
    "        \n",
    "        train_losses = filter(lambda itm: itm['epoch_num'] == epoch_num, self.recorder.lut['train_losses'])\n",
    "        # valid_losses = filter(lambda itm: itm['epoch_num'] == epoch_num, self.recorder.lut['valid_losses'])\n",
    "        \n",
    "        def _redux(loss_list_struct):\n",
    "            loss_list_struct = [itm['loss'] for itm in loss_list_struct]\n",
    "            loss_list_struct = np.asarray(loss_list_struct)\n",
    "            return loss_list_struct\n",
    "        \n",
    "        average_train_loss = _redux(train_losses).mean(axis=0)\n",
    "\n",
    "        self.recorder.lut['train_losses'].append({\n",
    "            'epoch_num':epoch_num,\n",
    "            'iter_num':-1,\n",
    "            'loss':average_train_loss,\n",
    "        })\n",
    "\n",
    "                \n",
    "        self.recorder.lut['epoch']+=1\n",
    "        self.recorder.lut['train_iter']=0\n",
    "        self.recorder.lut['valid_iter']=0\n",
    "        \n",
    "        eval_results = self.eval_metrics()\n",
    "        \n",
    "        eval_results['train']['loss'] = [_post_process_result(average_train_loss),]\n",
    "        \n",
    "        t = []\n",
    "        for k,d in eval_results.items():\n",
    "            for kk,v in d.items():\n",
    "                t.append((kk,k))\n",
    "        t = sorted(t, key=lambda tup: (tup[0],tup[1]))\n",
    "        cols = pd.MultiIndex.from_tuples(t)\n",
    "        df = pd.DataFrame(columns = cols)\n",
    "        for k,d in eval_results.items():\n",
    "            for kk,v in d.items():\n",
    "                df[(kk,k)] = [v,]\n",
    "        df.index = [epoch_num+1]\n",
    "        df = df.rename_axis([\"\",\"EPOCH\"], axis=\"columns\")\n",
    "\n",
    "        pd.set_option('max_colwidth', 500)\n",
    "        s = df.to_string(na_rep='').split(\"\\n\")\n",
    "        if ep_num==0:\n",
    "            self.logger(\"\\n\".join(s))\n",
    "        else: self.logger(\"\\n\".join(s[2:]))\n",
    "        return \n",
    "    \n",
    "    def eval_metrics(self):\n",
    "        \"\"\"\n",
    "           Evaluate model on set of data\n",
    "        \"\"\"\n",
    "        eval_results = defaultdict(dict)\n",
    "        \n",
    "        for eval_set_name in self.eval_sets:\n",
    "            eval_results[eval_set_name] = {}\n",
    "            if eval_set_name in self.data:\n",
    "                eval_results[eval_set_name] = _eval_fns_on_ds(\n",
    "                    self,\n",
    "                    self.data[eval_set_name],\n",
    "                    self.eval_sets[eval_set_name],\n",
    "                    info = eval_set_name\n",
    "                )\n",
    "            else:\n",
    "                eval_results[eval_set_name]['error'] = \"na in ds\"\n",
    "        return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Learner(_base_learner):\n",
    "    def __init__(self, model, data, loss_func, optimizer,scheduler=None, *args, **kwargs):\n",
    "        self.model,self.data,self.loss_func,self.optimizer,self.scheduler = model,data,loss_func,optimizer,scheduler\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, num_epochs=5):\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # train loop\n",
    "            _dataset_forward_pass(\n",
    "                ProgressBar(self.data['train'], status = \"{}/{}\".format(epoch+1,num_epochs)),\n",
    "                self.model,\n",
    "                self.loss_func,\n",
    "                True, # is_train\n",
    "                self.on_batch_begin,\n",
    "                self.on_batch_end,\n",
    "                self.on_backward_begin,\n",
    "                self.on_backward_end,\n",
    "                self.optimizer,\n",
    "                self.device\n",
    "            )        \n",
    "\n",
    "            \n",
    "            # clean up\n",
    "            self.on_epoch_end(epoch)\n",
    "            \n",
    "    def predict(self, dl, info=''):\n",
    "        # TODO: make better !!\n",
    "        P,Y,L = [],[],[]\n",
    "        def prediction_recorder_fn(xb,yb,pb,loss,is_train):\n",
    "            if isinstance(loss,(list,tuple)):\n",
    "                loss = [l.tolist() for l in loss]\n",
    "            else: \n",
    "                loss = [loss.tolist(),]\n",
    "            L.append(loss)\n",
    "            P.append(pb)\n",
    "            Y.append(yb)\n",
    "            \n",
    "        def _post(arr):\n",
    "            if isinstance(arr[0],(list,tuple)):\n",
    "                arr = list(zip(*arr)) \n",
    "                arr = [_post2(a) for a in arr]\n",
    "                return arr\n",
    "            if isinstance(arr[0],(torch.Tensor)):\n",
    "                return np.concatenate([a.cpu().numpy() for a in arr])\n",
    " \n",
    "        _dataset_forward_pass(\n",
    "            ProgressBar(dl,info),\n",
    "            self.model,\n",
    "            self.loss_func,\n",
    "            False, # is_train\n",
    "            self.on_batch_begin,\n",
    "            prediction_recorder_fn,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            self.device\n",
    "        )  \n",
    "        \n",
    "               \n",
    "        Y = _post(Y)\n",
    "        P = _post(P)\n",
    "   \n",
    "            \n",
    "        return P,Y,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# Metrics\n",
    "def accuracy(predictions, grnd_truth):\n",
    "    p = predictions\n",
    "    g = grnd_truth\n",
    "    p = p.argmax(axis=1)\n",
    "    return (p==g).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01nabla2.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 05_progressbar.ipynb.\n",
      "Converted 99_tests.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
