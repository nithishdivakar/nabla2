{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from pathlib import Path\n",
    "from typing import Any, Union, Tuple, Callable, TypeVar, Generic\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from nabla2.core import force_list\n",
    "from nabla2.progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# constants\n",
    "DISPLAY_DECIMALS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _set_device(device):\n",
    "    if device is None:\n",
    "        return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _prepare_batch(tb, device):\n",
    "    tb = force_list(tb)\n",
    "    tb = [t.to(device) if t is not None else None for t in tb]\n",
    "    return tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _batch_forward_pass(device, model, xb, yb,  compute_loss, loss_func=None):\n",
    "    # takes care of multiple input and or multiple output from model.\n",
    "    xb = _prepare_batch(xb, device)\n",
    "    pb = model(*xb)\n",
    "\n",
    "    if compute_loss  and (loss_func is not None):\n",
    "        yb = _prepare_batch(yb, device)\n",
    "        loss = loss_func(pb, *yb)\n",
    "    else:\n",
    "        loss=None\n",
    "\n",
    "    return loss, pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _show_loss_on_progress_bar(ds,loss)-> None:\n",
    "    if isinstance(ds,ProgressBar):\n",
    "        if isinstance(loss,(list,tuple)):\n",
    "            loss_val = loss[0].item()\n",
    "        else:\n",
    "            loss_val = loss.item()    \n",
    "        ds.set_status(\"{:4.2f}\".format(loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _dataset_forward_pass(dataset, model, loss_func, is_train:bool, batch_preprocess_fn, prediction_recorder_fn, \n",
    "                          loss_pre_process_fn, gradient_post_proces_fn, optimizer, device )->None:\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    for data_batch in dataset:\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            phase = 'train' if is_train==True else 'eval'\n",
    "            xb, yb = batch_preprocess_fn(data_batch, phase=phase)\n",
    "            loss, pb = _batch_forward_pass(device, model, xb, yb, compute_loss=True, loss_func=loss_func)\n",
    "            prediction_recorder_fn(xb,yb,pb,loss,is_train=is_train)\n",
    "            _show_loss_on_progress_bar(dataset, loss)\n",
    "            if is_train:                \n",
    "                loss = loss_pre_process_fn(loss)\n",
    "                loss.backward()\n",
    "                gradient_post_proces_fn()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _recorder():\n",
    "    lut = {\n",
    "        'epoch':0,\n",
    "        'train_iter':0,\n",
    "        'valid_iter':0,\n",
    "        'train_losses':[],\n",
    "        'valid_losses':[],\n",
    "    }\n",
    "    \n",
    "    def get_loss_df(self):\n",
    "        train_loss_df = pd.DataFrame(self.lut['train_losses'])\n",
    "        valid_loss_df = pd.DataFrame(self.lut['valid_losses'])\n",
    "        \n",
    "        def _cleanup(df):        \n",
    "            f = pd.DataFrame(df.loss.to_list(),index= df.index)\n",
    "            f.columns = [f\"loss_{idx}\" for idx,col in enumerate(f.columns)]\n",
    "            x = pd.concat([df,f],axis=1)\n",
    "            x=x.drop(columns=['loss'])\n",
    "            return x\n",
    "        \n",
    "        return _cleanup(train_loss_df),_cleanup(valid_loss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _logger():\n",
    "    folder = None\n",
    "    log_file = None\n",
    "    \n",
    "    def __init__(self, exp_folder=None):\n",
    "        if exp_folder is not None:\n",
    "            self.folder = exp_folder\n",
    "        else:\n",
    "            self.folder = Path(\"debug\")\n",
    "            \n",
    "        self.log_file = self.folder/\"log.txt\"\n",
    "        \n",
    "        self.folder.mkdir(parents=True,exist_ok=True)\n",
    "        \n",
    "        self.__call__(\">\"*40)\n",
    "        self.__call__(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        \n",
    "    def __call__(self, msg):\n",
    "        if self.log_file:\n",
    "            with self.log_file.open(\"a\") as f:\n",
    "                f.write(msg)\n",
    "                f.write(\"\\n\")\n",
    "        sys.stdout.write(msg+\"\\n\")\n",
    "        sys.stdout.flush()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class _base_learner():\n",
    "    eval_sets = defaultdict(list)\n",
    "    def __init__(self, device, exp_folder=None, eval_sets:dict=None):\n",
    "        \n",
    "        self.device = _set_device(device)\n",
    "        self.recorder = _recorder()\n",
    "        self.logger = _logger()\n",
    "        \n",
    "        self.eval_sets['valid'] = [] # do validation always. \n",
    "        \n",
    "        for set_name in eval_sets:\n",
    "            self.eval_sets[set_name].extend(force_list(eval_sets[set_name]))\n",
    "        \n",
    "    def on_backward_begin(self,loss):\n",
    "        if isinstance(loss,(list,tuple)): \n",
    "            return loss[0]\n",
    "        else:\n",
    "            return loss\n",
    "        \n",
    "    def on_backward_end(self):\n",
    "        return \n",
    "\n",
    "    def on_batch_begin(self, data_batch, phase='train'):\n",
    "        # do somthing . with a batch of data\n",
    "        # print(xb.mean(),xb.std(),xb.shape)\n",
    "        xb,yb = data_batch['x'] , data_batch['y'] \n",
    "        return xb,yb\n",
    "    \n",
    "    def on_batch_end(self,xb,yb,pb,loss,is_train=True):\n",
    "        # log losses    \n",
    "        if is_train:\n",
    "            lst = self.recorder.lut['train_losses']\n",
    "            epoch_num = self.recorder.lut['epoch']\n",
    "            iter_num  = epoch_num*len(self.data['train'])+self.recorder.lut['train_iter']\n",
    "        else:\n",
    "            lst = self.recorder.lut['valid_losses']\n",
    "            epoch_num = self.recorder.lut['epoch']\n",
    "            iter_num  = epoch_num*len(self.data['valid'])+self.recorder.lut['valid_iter']\n",
    "        if isinstance(loss,(list,tuple)):\n",
    "            loss = [l.tolist() for l in loss]\n",
    "        else: \n",
    "            loss = [loss.tolist(),]\n",
    "        \n",
    "        def get_lr():\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                return param_group['lr']\n",
    "        \n",
    "        lst.append({\n",
    "            'epoch_num':epoch_num,\n",
    "            'iter_num':iter_num,\n",
    "            'loss':loss,\n",
    "            'lr':get_lr(),\n",
    "        })\n",
    "        \n",
    "        # increment counters\n",
    "        if is_train==True: \n",
    "            self.recorder.lut['train_iter']+=1\n",
    "        else:\n",
    "            self.recorder.lut['valid_iter']+=1\n",
    "    \n",
    "    def on_epoch_end(self, ep_num):\n",
    "        epoch_num= self.recorder.lut['epoch']\n",
    "        \n",
    "        train_losses = filter(lambda itm: itm['epoch_num'] == epoch_num, self.recorder.lut['train_losses'])\n",
    "        valid_losses = filter(lambda itm: itm['epoch_num'] == epoch_num, self.recorder.lut['valid_losses'])\n",
    "        \n",
    "        def _redux(loss_list_struct):\n",
    "            loss_list_struct = [itm['loss'] for itm in loss_list_struct]\n",
    "            loss_list_struct = np.asarray(loss_list_struct)\n",
    "            return loss_list_struct\n",
    "        \n",
    "        average_train_loss = _redux(train_losses).mean(axis=0)\n",
    "        average_valid_loss = _redux(valid_losses).mean(axis=0)\n",
    "\n",
    "        self.recorder.lut['train_losses'].append({\n",
    "            'epoch_num':epoch_num,\n",
    "            'iter_num':-1,\n",
    "            'loss':average_train_loss,\n",
    "        })\n",
    "\n",
    "        self.recorder.lut['valid_losses'].append({\n",
    "            'epoch_num':epoch_num,\n",
    "            'iter_num':-1,\n",
    "            'loss':average_valid_loss,\n",
    "        })\n",
    "        \n",
    "        def _pf(loss_list):\n",
    "            return np.around(loss_list,DISPLAY_DECIMALS)\n",
    "        msg = [f\"EPOCH {epoch_num+1}\",f\"{_pf(average_train_loss)}\",f\"{_pf(average_valid_loss)}\"]\n",
    "        \n",
    "        self.recorder.lut['epoch']+=1\n",
    "        self.recorder.lut['train_iter']=0\n",
    "        self.recorder.lut['valid_iter']=0\n",
    "        \n",
    "        eval_results = self.eval_metrics()\n",
    "        \n",
    "        eval_results['train']['loss'] = [_pf(average_train_loss),]\n",
    "        \n",
    "        t = []\n",
    "        for k,d in eval_results.items():\n",
    "            for kk,v in d.items():\n",
    "                t.append((kk,k))\n",
    "        t = sorted(t, key=lambda tup: (tup[0],tup[1]))\n",
    "        cols = pd.MultiIndex.from_tuples(t)\n",
    "        df = pd.DataFrame(columns = cols)\n",
    "        for k,d in eval_results.items():\n",
    "            for kk,v in d.items():\n",
    "                df[(kk,k)] = [v,]\n",
    "        df.index = [epoch_num+1]\n",
    "        df = df.rename_axis([\"\",\"EPOCH\"], axis=\"columns\")\n",
    "\n",
    "        pd.set_option('max_colwidth', 500)\n",
    "        s = df.to_string(na_rep='').split(\"\\n\")\n",
    "        if ep_num==0:\n",
    "            self.logger(\"\\n\".join(s))\n",
    "        else: self.logger(\"\\n\".join(s[2:]))\n",
    "        return \n",
    "    \n",
    "    def eval_metrics(self):\n",
    "        \"\"\"\n",
    "           Evaluate model on set of data\n",
    "        \"\"\"\n",
    "        def _post_process_eval_result(result):\n",
    "            \n",
    "            if isinstance(result,(float,int)):\n",
    "                return np.around(result,DISPLAY_DECIMALS)\n",
    "            \n",
    "            elif isinstance(result,(np.float32,np.float64)):\n",
    "                return np.around(result,DISPLAY_DECIMALS)\n",
    "\n",
    "            elif isinstance(result,(list,tuple)):\n",
    "                return [_post_process_eval_result(t) for t in result]\n",
    "\n",
    "            elif isinstance(result, np.ndarray):\n",
    "                return np.around(results,DISPLAY_DECIMALS)\n",
    "\n",
    "            return result\n",
    "\n",
    "        eval_results = defaultdict(dict)\n",
    "        \n",
    "        for eval_set_name in self.eval_sets:\n",
    "            eval_results[eval_set_name] = {}\n",
    "            if eval_set_name in self.data:\n",
    "                predictions,ground_truth,loss_values = self.predict(self.data[eval_set_name],eval_set_name)\n",
    "                for eval_fn in self.eval_sets[eval_set_name]:\n",
    "                    \n",
    "                    try:\n",
    "                        name = eval_fn.__name__\n",
    "                    except:\n",
    "                        name = eval_fn.__class__.__name__\n",
    "                    \n",
    "                    try:\n",
    "                        ms = eval_fn(predictions, ground_truth)\n",
    "                    except Exception as e:\n",
    "                        ms = str(e)\n",
    "                    ms = _post_process_eval_result(ms)\n",
    "                    eval_results[eval_set_name][name] = ms\n",
    "                \n",
    "                # always record loss value irrespective of other functions\n",
    "                # not much compute is wasted in doing this as we already \n",
    "                # have the numbers. \n",
    "                out = np.asarray(loss_values)\n",
    "                out = out.mean(axis=0)\n",
    "                eval_results[eval_set_name]['loss'] = np.around(out,DISPLAY_DECIMALS)\n",
    "                \n",
    "            else:\n",
    "                eval_results[eval_set_name]['error'] = \"na in ds\"\n",
    "\n",
    "        return eval_results\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Learner(_base_learner):\n",
    "    def __init__(self, model, data, loss_func, optimizer,scheduler=None, *args, **kwargs):\n",
    "        self.model,self.data,self.loss_func,self.optimizer,self.scheduler = model,data,loss_func,optimizer,scheduler\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, num_epochs=5):\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # train loop\n",
    "            _dataset_forward_pass(\n",
    "                ProgressBar(self.data['train'], status = \"{}/{}\".format(epoch+1,num_epochs)),\n",
    "                self.model,\n",
    "                self.loss_func,\n",
    "                True, # is_train\n",
    "                self.on_batch_begin,\n",
    "                self.on_batch_end,\n",
    "                self.on_backward_begin,\n",
    "                self.on_backward_end,\n",
    "                self.optimizer,\n",
    "                self.device\n",
    "            )        \n",
    "\n",
    "            \n",
    "            # clean up\n",
    "            self.on_epoch_end(epoch)\n",
    "            \n",
    "    def predict(self, dl, info=''):\n",
    "        # TODO: make better !!\n",
    "        P,Y,L = [],[],[]\n",
    "        def prediction_recorder_fn(xb,yb,pb,loss,is_train):\n",
    "            if isinstance(loss,(list,tuple)):\n",
    "                loss = [l.tolist() for l in loss]\n",
    "            else: \n",
    "                loss = [loss.tolist(),]\n",
    "            L.append(loss)\n",
    "            P.append(pb)\n",
    "            Y.append(yb)\n",
    "            \n",
    "        \n",
    "        _dataset_forward_pass(\n",
    "            ProgressBar(dl,info),\n",
    "            self.model,\n",
    "            self.loss_func,\n",
    "            False, # is_train\n",
    "            self.on_batch_begin,\n",
    "            prediction_recorder_fn,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            self.device\n",
    "        )  \n",
    "        \n",
    "        def _post(arr):\n",
    "            if isinstance(arr[0],(list,tuple)):\n",
    "                arr = list(zip(*arr)) \n",
    "                arr = [_post2(a) for a in arr]\n",
    "                return arr\n",
    "            if isinstance(arr[0],(torch.Tensor)):\n",
    "                return np.concatenate([a.cpu().numpy() for a in arr])\n",
    "        \n",
    "        Y = _post(Y)\n",
    "        P = _post(P)\n",
    "   \n",
    "            \n",
    "        return P,Y,L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 02_learner.ipynb.\n",
      "Converted 05_progressbar.ipynb.\n",
      "Converted 99_tests.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
